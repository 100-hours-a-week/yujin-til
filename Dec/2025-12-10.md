## 📆 2025-12-10 ~ 12

### 🔔 스크럼

- 부하테스트 특강 복습 및 정리
- 부하테스트 시작
  - 단일 인스턴스에 올려서 성능 확인
  - 아키텍쳐 구상

### 🚀 Today I Learned

#### 우리 서비스, 몇명까지 버틸 수 있을까?

- 성능 테스트 : 트래픽이 많은 상황에서 발생하는 문제를 사전에 찾기 위한 작업
  - 이 안에 부하테스트 속해있음
- 성능 테스트 흐름
  - 시나리오 설정
  - 목표 설정 계산
    - 일일 총 요청 계산
    - RPS 계산
      - 피크 집중률, 안전 여유분 고려
    - 응답 시간 목표 설정 → 연구결과, 경쟁사 분석
    - VU(가상 유저) 계산
  - 테스트 도구 선정
    - Jmeter, k6, locust, vegeta, Artillery
- 이번 부하테스트에서 부하 주입 전략
  - E2E : 기능 검증용
    - E2E로만 부하주입하면 리소스 굉장히 많이써야함.
  - API/Websocket : 부하주입 → k6로 하는걸 추천
- 정리
  - 목표를 데이터 기반으로 설정 → 코드 최적화 → 인프라 스케일링으로 확장
- **팀별로 프로메테우스/그라파나 적용 추천**
- 이번 부하테스트의 목표 설정
  - 심사기준에서의 p95,p99와 vu를 점진적으로 늘려서 확인

#### 성능지표 확인방법

- 성능관측이 왜 필요할까?
  - 모호한 표현들(ex. 캐싱적용하면 빨라질거같은데?)을 숫자로 설명가능하게 하여 제한 된 시간 내 우선순위를 결정할 수 있어서
  - 성능관측은 단순 상태 모니터링이 아니라, 제한된 자원을 어디에 쓸지 결정하는 전략적 판단 도구임
- APM : 애플리케이션 성능 측정 및 문제 진단도구
- Obervability(관측성) : 더 넓은 개념, 시스템 상태를 이해하는 능력
  - metrics(지표) : 수치화된 데이터. 요청 수, 레이턴시, CPU 등
  - Logs : 이벤트에 대한 텍스트 기록
    - 로그 구분을 어떻게할까?
      - 오류/비즈니스 로직 별로 따로 수집
      - 성공 로그는 필요하지않는 이상 잘 안남김 → CS하면서 추가됨
  - Traces : 분산된 요청의 전체 흐름 추적
- 성능지표는 여러 관점을 오가며 해석
  - 사용자 관점(증상확인) → 애플리케이션 관점(병목 후보) → JVM/런타임/인프라 관점(원인 확증)
  - 주의! CPU 사용률이 높다고해서 반드시 CPU원인이 아닐수도있음
- 요청 분포화 핫스팟 - “어디에 부하가 몰리나?”
  - 얼마나 느린가(Lantency)와 얼마나 자주 실행하는가(Frequency)를 함께 고려
- 관측에서 최적화로
  - 엔지니어링 사이클 반복 : 관측 → 가설 → 실험 → 검증

### 🔥 부하테스트 준비 순서

#### 0. 아키택쳐 구상 전 준비 단계

- 본격적인 분산 부하 테스트에 앞서, 단일 EC2 인스턴스 환경에서 k6 브레이크 포인트 테스트를 먼저 수행 → 그 결과 대부분의 API가 Disk I/O 병목으로 터지는 현상을 확인 → 이를 통해, 단일 인스턴스에서의 성능 한계 인지, 수평 확장을 전제로 한 테스트 환경 구성이 필요하다고 판단

- 사용 가능한 리소스는 t3.small EC2 인스턴스 최대 20대로 제한되어 있었기 때문에,
  Back-End, Front-End, DB, Redis, 모니터링 서버 등 역할별로 인스턴스를 분리하여 배치하는 전략을 선택

- 초기에는 단일 인스턴스에서 모든 서비스를 Docker로 함께 실행했으나, 컨테이너 수가 과도하게 증가하면서 자원을 너무 많이 잡아먹음 → 각 역할별 EC2 환경에서는 Docker 사용하지 않고, 단일 인스턴스 실험 단계에서만 Docker를 활용하기로 결정. 백엔드와 프론트엔드 모두 로컬에서 빌드한 결과물을 서버에 배포한 뒤 실행만 수행하는 방식으로 배포 전략을 변경

- Route 53, Application Load Balancer(ELB), S3 사용이 가능한 환경을 기반으로 인프라를 설계

- 최종적으로, 단일 인스턴스 브레이크 포인트 테스트 결과를 기준 데이터로 확보한 후,
  해당 결과를 토대로 분산 환경에서의 부하 테스트를 진행할 수 있도록 준비를 마무리

#### 1. 초기 아키택쳐 구상

<img width="691" height="292" alt="Image" src="https://github.com/user-attachments/assets/e02f9182-8a6c-40c8-b418-4c3c6840608d" />

- **Backend 서버 7대** : CRUD 중심의 고정형 트래픽을 안정적으로 처리하기 위해 가장 많은 인스턴스를 배치
- **WebSocket 서버 4대** : 실시간 연결로 인한 지속적인 커넥션 유지 부담을 고려하여 별도 도메인으로 분리 → 이를 통해 실시간 커넥션 유지로 인한 부하가 일반 API 처리에 영향을 주지 않도록 설계
- **Frontend 서버 3대** : SSR(Server-Side Rendering)만 수행하며, 정적 파일은 S3로 분리되어 상대적으로 적은 인스턴스로 구성
- **Redis 서버 2대** : WebSocket 서버를 다수로 분리하면서, 서버 간 상태 공유 및 메시지 전달을 위해 Redis를 핵심 인프라로 사용할 것으로 예상. 이에 따라 Redis를 2대로 구성하였으나, 실제 구성에서는 1대만 Pub/Sub 및 캐시 용도로 사용되었고, 나머지 1대는 장애 상황을 대비한 예비(백업) 인스턴스 성격에 가까운 역할을 수행
- **DB 서버 3대** : EC2 한대에 다 올려서 BreakPoint Test헤봤을때, DB I/O 터지는 걸 모니터링 도구로 확인하여 1대로 두는건 아닌것 같다고 판단 → 읽기/쓰기용을 따로 두자라고 생각했고, 소켓으로 채팅 목록 등을 불러올때 읽기가 많은거 같아서 읽기를 2대, 쓰기 1대로 Replica Set 구성 → secondary 죽으면 primary로 승격되게 설정하여 가용성 확보
- **모니터링 서버 1대** : Prometheus와 Grafana를 활용한 모니터링 서버로 구성. Node Exporter를 통해 각 EC2 인스턴스의 CPU, Memory, Disk, Network 지표를 수집하고,
  애플리케이션 단에서는 Spring Boot Actuator 및 커스텀 메트릭을 통해 HTTP 요청, WebSocket 이벤트, DB 및 Redis 관련 메트릭을 Prometheus로 수집 → 이를 통해 부하 테스트 중 발생하는 병목 지점을 실시간으로 관측하고, 시스템 한계 도달 시점을 데이터 기반으로 분석할 수 있도록 설계

### 최종 아키텍쳐 및 성능 최적화 전략 정리

<img width="611" height="653" alt="Image" src="https://github.com/user-attachments/assets/2a99f127-5695-4e6d-9fad-e8b6d7795e27" />

#### 1. 서비스 도메인 별 서버 분리 & 수평 확장

- 전체 20대의 EC2 인스턴스를 분리 배치하여, 각 도메인이 자신의 트래픽 특성에 맞게 리소스를 사용할 수 있도록 설계
  - **Backend 서버 12대** : 소켓 연결 안정화 대기하는 타임아웃이 다른 애들에 비해 오래걸려서 분리해놨던 소켓을 백엔드 인스턴스에 합쳐서 기존 소켓 서버를 백엔드에 추가
  - **Frontend 서버 3대**
  - **Redis 서버 1대** : 기존 1대는 백업용이라 현재는 백업보다는 더 많은 트래픽을 견디는게 우선이라 생각해서 백엔드에 한대 추가
  - **DB 서버 3대**
  - **모니터링 서버 1대**

#### 2. 프론트엔드 정적 파일 분리(S3)

- 정적 파일을 S3로 분리함으로써 FE 서버는 **SSR 로직만 수행**하게 되어 부하 테스트 시 서버 CPU/메모리 점유율이 크게 줄어들것을 예상

#### 3. MongoDB ReplicaSet(3대) 구성 → 고가용성 + I/O 안정성 확보

- DB I/O 부담을 줄이를 완화하기 위해 MongoDB 3대를 구성한 ReplicaSet(1 Primary + 2 Secondary) 을 구성

### 🔍 부하테스트 결과 분석

- 테스트 개요

  - 최대 안정 처리 사용자 수: 약 210명 × 3 그룹
  - 점진적 사용자 증가 방식으로 브레이크 포인트 확인

- 장애 발생 지점

  - DB 인스턴스 3번(읽기)에서 System Load 100 이상으로 급상승
  - 해당 시점부터 응답 지연 및 요청 실패 발생
  - 결과적으로 DB 요청 큐가 가득 차며 병목 발생, 시스템이 정상적으로 요청을 처리하지 못한 것으로 판단됨

- 원인 분석

  - 애플리케이션 서버(Back-End)는 수평 확장을 통해 어느 정도 부하를 분산할 수 있었으나, DB는 단일 병목 지점(Single Bottleneck)으로 작동함
  - 특히 읽기/쓰기 요청이 특정 DB 인스턴스(3번)에 집중되면서,

    - CPU/IO 한계 도달
    - DB 내부 큐 대기 증가
    - 전체 서비스 응답 성능 저하로 이어짐

  - 이는 애플리케이션 계층이 아니라 데이터 계층이 전체 시스템의 한계를 결정하고 있음을 알 수 있음

- 개선 방향

  - DB 스케일 전략 재검토

    - 단순 스케일업(인스턴스 사양 증가)보다는, 읽기/쓰기 분리를 명확히 하거나, 필요 시 스케일아웃 구조로 전환하는 방안 고려

  - 캐시 전략 강화
    - Redis를 활용한 조회 결과 캐싱을 통해 DB로 직접 유입되는 트래픽을 줄이는 것이 필수적임을 확인
    - 특히 반복 조회되는 데이터는 DB 접근 전에 캐시에서 처리하도록 구조 개선 필요

- DB 접근 자체를 줄이는 설계
  - 클라이언트에서 미리 전달 가능한 정보는 요청 시 함께 전달하여 불필요한 DB 조회를 최소화

### 🗨️ 오늘의 회고

이번 부하 테스트는 2박 3일 동안 클라우드, 풀스택, 인공지능 파트가 처음으로 한 팀을 이루어 각자의 역할을 수행한 경험이었다.
그중에서도 클라우드 파트는 인프라가 먼저 준비되지 않으면 테스트 자체를 시작할 수 없기 때문에,
팀 전체 흐름을 좌우한다는 점에서 부담이 컸다.

특강에서 배운 내용을 바탕으로 최소한의 테스트 환경을 먼저 구성하고,
이를 기반으로 아키텍처를 설계한 뒤 직접 실험해보는 과정을 반복하면서
이론으로만 알고 있던 개념들이 실제 시스템에서 어떻게 드러나는지를 체감할 수 있었다.
특히 부하 테스트 중 발생하는 병목 지점을 파악하기 위해서는
모니터링이 선택이 아니라 필수라는 점을 깨닫게 되었고, 단순히 모니터링 서버를 세팅하는 것에서 끝나는 것이 아니라 각 지표가 무엇을 의미하는지 해석하고 원인과 연결하는 과정이 생각보다 쉽지 않았다.

또한 이번 경험을 통해
무조건 Docker를 사용하거나, 무조건 서버를 많이 늘린다고 해서 성능이 좋아지는 것은 아니다라는 사실을 알게 되었다. 오히려 서비스 특성과 트래픽 패턴을 고려하지 않은 과도한 구성은
리소스 낭비나 새로운 병목을 만들 수 있다는 점을 실험을 통해 확인할 수 있었다.

여러모로 처음 해보는 시도들이 많았지만,
실제로 설계하고, 측정하고, 원인을 분석하는 과정을 거치며
스스로 한 단계 성장했음을 느낄 수 있었다.
다음에 비슷한 기회가 온다면,
실험 → 측정 → 변경의 사이클을 더 짧게 가져가며
보다 빠르게 병목을 찾고, 더 나은 성능을 내는 아키텍쳐를 설계해보고 싶다.
