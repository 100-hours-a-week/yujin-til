## 📆 2025-11-19

### 🔔 스크럼

- 컨테이너 네트워크, 프록시 강의 복습 및 정리
- 이번주 과제 EC2 생성하여 진행

### 🚀 Today I Learned

#### Bridge Network

- 컨테이너들이 동일한 호스트 내에서 서로 통신할 수 있도록 연결해주는 도커의 기본 가상 네트워크
- 각 컨테이너에 독립적인 가상 네트워크 인터페이스를 제공 → 네트워크 트래픽 격리 → 기본적으로 내부 IP주소를 부여받음
- 기본적으로 생성되는 Bridge Network가 있지만, 커스텀 가능함
- 사용이유 : 컨테이너 간 통신을 격리된 네트워크 환경에서 안전하고 간편하게 관리하기 위해서

#### Reverse Proxy

- 클라이언트의 요청을 대신 받아 서버에 전달하고, 서버의 응답을 클라이언트로 반환하는 중개역할을 하는 서버
- 이 역할이 가능한것
  - API Gateway
  - ELB
  - EC2(Nginx)
  - etc..
- 사용이유
  - 서버 보안 강화 : 각 WAS들이 엔드포인트라면 WAS에 보안처리를 해줘야하는데, 리버스 프록시 두면 걔만 보안해주면됨
  - 로드 밸런싱 : 여러 대의 서버로 요청을 분산시켜 한 서버에 과부하가 걸리는것 방지 가능
  - SSL 터미네이션 : 리버스 프록시에서 SSL 벗겨버리고, 벗겨버린 HTTP요청을 나눠줌. 처리를 한 다음에도 내부망에서도 HTTP로 주고, 리버스 프록시에서 SSL layer 다시 씌워서 밖으로 나감 → 인증서 관리 중앙화 & 암/복호화 부담을 프록시에 오프로드해 백엔드 부하를 줄일 수 있음
  - 캐싱 및 압축 : 자주 요청되는 데이터들을 프록시가 캐싱해두어 클라이언트 요청에 신속 응답 가능
- Q : 근데 리버스 프록시로 관리하면 단일 장애지점이 되는거 아닌가? 이걸 감수하고 사용하는건가?
  - SPOF : 단일 장애 지점 → 복구가 불가능한 지점
    - WS, WAS는 오토스케일링 그룹이라 복구가능
    - RDS도 Master, Slave 여러개였을때, Master죽으면 Slave하나를 Master로 승격시킴
    - ELB, SS는 죽으면 끝임 → SPOF임
  - 가용성이 높은 인프라 구성 = SPOF적은 인프라 구성 → 비쌈 → 그럼에도 쓰는 이유는?
  - A : 사용했을때의 장애 발생 확률 < 안사용했을때의 장애 발생확률이라서
    - 사용안하면, 각 WAS가 SSL을 벗기고, 인증/갱신해야하고 하니까 관심사 분리 안되고, 보안도 잘안됨
- Reverse Proxy vs Forward Proxy
  | 특징 | Reverse Proxy | Forward Proxy |
  | --------- | ----------------------------------- | ----------------------------------------- |
  | 위치 | 서버 앞에 위치 | 클라이언트 앞에 위치 |
  | 주요 역할 | 보안 강화, 로드 밸런싱, SSL 처리 등 | 웹 필터링, 익명성 유지, 지역 제한 우회 등 |
  | ex | Nginx, Apache HTTP Server, Caddy .. | VPN .. |

### 🔥 오늘의 도전 과제와 해결 방법

- 이번주 과제가 EC2에 전체 서비스 Docker Compose 배포(프론트엔드, 백엔드, MySQL, Nginx, Portainer, Private Registry까지)이다. EC2 환경에서 Portainer + Private Docker Registry + Nginx + HTTPS 구축까지 이어지는 전체 인프라 구성을 완성하는 과정에서 여러 문제가 발생했다. 특히 Portainer WebSocket 지원, certbot 충돌, nginx 포트 충돌, 도커 이미지 아키텍처 문제 등 “실제 배포 환경에서 자주 터지는 문제”들을 하나씩 해결했다.

1. Nginx와 Portainer 간 WebSocket 오류

   - Portainer는 실시간 로그, Exec 터미널 등 WebSocket 기반 기능이 많기 때문에 Nginx reverse proxy 시 추가 설정이 필수라는 걸 알게 됐다. 해당 설정이 없으면 터미널 연결 실패, 실시간 로그 미동작 등의 문제가 발생한다.

2. certbot 플러그인 충돌로 인한 HTTPS 인증 실패

   - nginx 플러그인이 인식되지 않는 문제가 있었는데, snap/apt 잔여 파일 충돌로 certbot 자체가 망가진 상태였다. → 불필요한 certbot 패키지와 잔여 파일 전부 제거 후 재설치하여 해결.

3. Nginx 포트 충돌로 Docker nginx 컨테이너 기동 실패

   - 시스템 nginx가 EC2에 설치돼 80/443 포트를 선점하면서 Docker nginx 컨테이너가 포트 바인딩을 못 하고 계속 재시작됐다. → system nginx를 완전 제거하고 해결.

4. Private Registry로 이미지 push 시 413 오류

   - Docker 레이어 파일이 큰데 Nginx 기본 설정에서 업로드 제한이 존재하여 413 Request Entity Too Large 에러 발생. → client_max_body_size 0; proxy_request_buffering off; 추가로 해결.

### 🗨️ 오늘의 회고

- 오늘은 “실제 배포 환경에서 왜 이런 문제가 생기는가”를 경험한 하루였다. 특히 WebSocket, 아키텍처 차이, 포트 충돌, 업로드 제한, certbot 충돌 같은 이슈들이 로컬에서 테스트해봤을때는 절대 보이지 않던 문제들인데, 배포 환경에서는 쉽게 발생하는 문제들이었다. 여러 문제를 하나씩 해결해가면서 배포 환경 특유의 복잡성과 중요성을 다시 느꼈고, 쉽지 않았지만 기본적인 인프라 구축 흐름을 직접 밟아본 만큼 앞으로는 더 견고하고 신뢰성 있는 환경을 설계해보고 싶다.
